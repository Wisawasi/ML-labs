{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab8_Google_AutoML.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imcinstitute/ML-labs/blob/main/LabVIII_Advanced_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lXh7q9tCdnv"
      },
      "source": [
        "# Project setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JCfyqMpCUAh"
      },
      "source": [
        "pip install tensorflow-gpu==2.0.0-rc0\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "tf.__version__  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xpjBRwuCjss"
      },
      "source": [
        "# Staging data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXYlYAiXCl3J"
      },
      "source": [
        "%%bigquery flights_df --verbose\n",
        "\n",
        "SELECT \n",
        "\n",
        "  -- Departure delay\n",
        "  departure_delay,\n",
        "    \n",
        "  -- Distance\n",
        "  distance,\n",
        "\n",
        "  -- Airlines\n",
        "  airline,\n",
        "    \n",
        "  -- Airports \n",
        "  departure_airport,\n",
        "  arrival_airport, \n",
        "\n",
        "  -- Date information\n",
        "  CAST(EXTRACT(DAYOFWEEK FROM departure_date) AS STRING) as departure_weekday,\n",
        "  CAST(EXTRACT(MONTH FROM departure_date) AS STRING) as departure_month,\n",
        "\n",
        " -- Target column\n",
        "  CASE WHEN (arrival_delay >= 15) THEN 1 ELSE 0 END as delayed\n",
        "FROM ( \n",
        "    \n",
        "    -- Inner Query\n",
        "    SELECT\n",
        "      \n",
        "      departure_delay,\n",
        "      ROUND(ST_DISTANCE(ST_GEOGPOINT(departure_lon, departure_lat), ST_GEOGPOINT(arrival_lon, arrival_lat))/1000) as distance,\n",
        "      airline,\n",
        "      arrival_airport,\n",
        "      departure_airport,\n",
        "      PARSE_DATE(\"%Y-%m-%d\", date) AS departure_date,\n",
        "      \n",
        "      arrival_delay\n",
        "      \n",
        "      \n",
        "    FROM\n",
        "      `bigquery-samples.airline_ontime_data.flights`\n",
        "    WHERE date >= '2009-01-01' \n",
        "    AND date <= '2009-12-31'\n",
        "    AND departure_delay > 0\n",
        "    \n",
        "  )\n",
        "%%bigquery high_traffic_airports --verbose\n",
        "\n",
        "SELECT * FROM\n",
        " \n",
        " (SELECT departure_airport as airport_code,\n",
        "  COUNT(*) as flights\n",
        "  \n",
        "  FROM\n",
        "    `bigquery-samples.airline_ontime_data.flights`    \n",
        "  \n",
        "  WHERE date >= '2009-01-01' \n",
        "    AND date <= '2009-12-31'\n",
        "    \n",
        "  GROUP BY departure_airport\n",
        "  ORDER BY airport_code)\n",
        "\n",
        "WHERE flights > 10000\n",
        "%%bigquery airline_codes --verbose\n",
        "\n",
        "SELECT DISTINCT(airline)\n",
        "  \n",
        "FROM\n",
        "    `bigquery-samples.airline_ontime_data.flights`\n",
        "    \n",
        "WHERE date >= '2009-01-01' \n",
        "    AND date <= '2009-12-31'\n",
        "    \n",
        "ORDER BY airline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuT6bRWtDBzG"
      },
      "source": [
        "# Explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK1krkk5DEwc"
      },
      "source": [
        "flights_df.shape\n",
        "\n",
        "flights_df.sample(n = 5)\n",
        "\n",
        "flights_df.dtypes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0QZclXDI0p"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QZI-Hk-DMkn"
      },
      "source": [
        "#Training-Testing-Split\n",
        "\n",
        "train_df = flights_df.sample(frac=0.8,random_state=123)\n",
        "test_df = flights_df.drop(train_df.index)\n",
        "\n",
        "\n",
        "#Check Label distribution\n",
        "print(round(flights_df.delayed.mean(),3)*100, '% delay in total dataset')\n",
        "\n",
        "#Build a tf.data.Dataset\n",
        "\n",
        "def dataframe_to_dataset(dataframe, labels = 'delayed', shuffle=True, batch_size=32):\n",
        "    # Creates a tf.data dataset from a Pandas Dataframe\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop(labels)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "train_ds = dataframe_to_dataset(train_df, batch_size=batch_size)\n",
        "test_ds = dataframe_to_dataset(test_df, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "#Build Features using tf.feature_column\n",
        "\n",
        "departure_delay_bins = [2, 3, 6, 9, 13, 19, 28, 44, 76]\n",
        "distance_bins = [600, 1200]\n",
        "airports_voc = high_traffic_airports['airport_code']\n",
        "airlines_voc = airline_codes['airline']\n",
        "weekdays_voc = ['1', '2', '3', '4', '5', '6', '7']\n",
        "months_voc = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
        "\n",
        "\n",
        "#Build the input pipeline\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "# bucketized columns\n",
        "distance = tf.feature_column.numeric_column(\"distance\")\n",
        "distance_buckets = tf.feature_column.bucketized_column(distance, boundaries = distance_bins)\n",
        "feature_columns.append(distance_buckets)\n",
        "\n",
        "departure_delay = tf.feature_column.numeric_column(\"departure_delay\")\n",
        "departure_delay_buckets = tf.feature_column.bucketized_column(departure_delay, boundaries = departure_delay_bins)\n",
        "feature_columns.append(departure_delay_buckets) \n",
        "\n",
        "# categorical columns\n",
        "arrival_airports = tf.feature_column.categorical_column_with_vocabulary_list('arrival_airport', airports_voc)\n",
        "arrival_airports_dummy = tf.feature_column.indicator_column(arrival_airports)\n",
        "feature_columns.append(arrival_airports_dummy)\n",
        "\n",
        "departure_airports = tf.feature_column.categorical_column_with_vocabulary_list('departure_airport', airports_voc)\n",
        "departure_airports_dummy = tf.feature_column.indicator_column(departure_airports)\n",
        "feature_columns.append(departure_airports_dummy)\n",
        "\n",
        "airlines = tf.feature_column.categorical_column_with_vocabulary_list('airline', airlines_voc)\n",
        "airlines_dummy = tf.feature_column.indicator_column(airlines)\n",
        "feature_columns.append(airlines_dummy)\n",
        "\n",
        "weekdays = tf.feature_column.categorical_column_with_vocabulary_list('departure_weekday', weekdays_voc)\n",
        "weekdays_dummy = tf.feature_column.indicator_column(weekdays)\n",
        "feature_columns.append(weekdays_dummy)\n",
        "\n",
        "months = tf.feature_column.categorical_column_with_vocabulary_list('departure_month', months_voc)\n",
        "months_dummy = tf.feature_column.indicator_column(months)\n",
        "feature_columns.append(months_dummy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bua6i6s1DNm2"
      },
      "source": [
        "# Defining non-distribution model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ub4NSUDpYT"
      },
      "source": [
        "#Define the feature layer\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "#Build Non-distributed model\n",
        "model_normal = tf.keras.models.Sequential([\n",
        "    \n",
        "    feature_layer,\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "    \n",
        "    ])\n",
        "\n",
        "model_normal.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "             )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yKPRkvYDuEJ"
      },
      "source": [
        "# Defining distribution model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUTWhynDxxi"
      },
      "source": [
        "#Creating the Mirrored Strategy instance\n",
        "distribute = tf.distribute.MirroredStrategy()\n",
        "\n",
        "#Build distributed model\n",
        "with distribute.scope():\n",
        "    model_distributed = tf.keras.models.Sequential([\n",
        "        feature_layer,\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "        ])\n",
        "\n",
        "    model_distributed.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy']\n",
        "                 )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJuZR66nD3_n"
      },
      "source": [
        "# Training model normal training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UutcR_twD8JO"
      },
      "source": [
        "start_time = time.time()\n",
        "history = model_normal.fit(train_ds, \n",
        "                    epochs = 5,\n",
        "                    callbacks = [tf.keras.callbacks.TensorBoard(\"logs/normal_training\")])\n",
        "print(\"Normal training took: {}\".format(time.time() - start_time))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZC1opJD9Gw"
      },
      "source": [
        "# Traning model distributed training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFaRI8SyEHu7"
      },
      "source": [
        "start_time = time.time()\n",
        "history = model_distributed.fit(train_ds,\n",
        "                    epochs = 5,\n",
        "                    callbacks = [tf.keras.callbacks.TensorBoard(\"logs/distributed_training\")])\n",
        "print(\"Distributed training took: {}\".format(time.time() - start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}